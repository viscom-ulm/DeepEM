<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="DEEP-EM TOOLBOX" />
  <meta property="og:description"
    content="Unlock the power of Deep Learning in Electron Microscopy with the DEEP-EM TOOLBOX standardized workflows for EM image analysis." />
  <meta property="og:url" content="URL OF THE WEBSITE" />
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200" />
  <meta property="og:image:height" content="630" />


  <meta name="twitter:title" content="DEEP-EM TOOLBOX">
  <meta name="twitter:description"
    content="Unlock the power of Deep Learning in Electron Microscopy with the DEEP-EM TOOLBOX standardized workflows for EM image analysis.">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="Deep Learning, Electron Microscopy, Data Analysis, Data Interpretation, Toolbox">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>DEEP-EM TOOLBOX</title>
  <link rel="icon" type="image/x-icon" href="static/images/icon.png">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">
  <style>
    .hidden {
      display: none;
    }

    .button-55 {
      align-self: center;
      background-color: #fff;
      background-image: none;
      background-position: 0 90%;
      background-repeat: repeat no-repeat;
      background-size: 4px 3px;
      border-radius: 15px 225px 255px 15px 15px 255px 225px 15px;
      border-style: solid;
      border-width: 2px;
      box-shadow: rgba(0, 0, 0, .2) 15px 28px 25px -18px;
      box-sizing: border-box;
      color: #41403e;
      cursor: pointer;
      display: inline-block;
      font-family: Neucha, sans-serif;
      font-size: 1rem;
      line-height: 23px;
      outline: none;
      padding: .75rem;
      text-decoration: none;
      transition: all 235ms ease-in-out;
      border-bottom-left-radius: 15px 255px;
      border-bottom-right-radius: 225px 15px;
      border-top-left-radius: 255px 15px;
      border-top-right-radius: 15px 225px;
      user-select: none;
      -webkit-user-select: none;
      touch-action: manipulation;
    }

    .button-55:hover {
      box-shadow: rgba(0, 0, 0, .3) 2px 8px 8px -5px;
      transform: translate3d(0, 2px, 0);
    }

    .button-55:focus {
      box-shadow: rgba(0, 0, 0, .3) 2px 8px 4px -6px;
    }

    .green-background {
      background-color: #dde9afff;
      /* Green background */
      padding: 20px;
      /* Padding inside the element */
      border-radius: 10px;
      /* Rounded corners */
      box-shadow: 0 4px 8px rgba(0, 0, 0, 0.2);
      /* Subtle shadow for depth */
      font-size: 16px;
      /* Font size */
      margin: 20px 0;
      /* Margin outside the element */
    }

    .red-background {
      background-color: #ffaaaaff;
      /* Green background */
      padding: 20px;
      /* Padding inside the element */
      border-radius: 10px;
      /* Rounded corners */
      box-shadow: 0 4px 8px rgba(0, 0, 0, 0.2);
      /* Subtle shadow for depth */
      font-size: 16px;
      /* Font size */
      margin: 20px 0;
      /* Margin outside the element */
    }

    .orange-background {
      background-color: #ffb380ff;
      /* Green background */
      padding: 20px;
      /* Padding inside the element */
      border-radius: 10px;
      /* Rounded corners */
      box-shadow: 0 4px 8px rgba(0, 0, 0, 0.2);
      /* Subtle shadow for depth */
      font-size: 16px;
      /* Font size */
      margin: 20px 0;
      /* Margin outside the element */
    }
  </style>

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  <script>
    function toggleVisibility(id_content, id_button) {
      var content = document.getElementById(id_content);
      var btn = document.getElementById(id_button);


      if (content.classList.contains('hidden')) {
        content.classList.remove('hidden');
        btn.innerHTML = "Show Less"
      } else {
        content.classList.add('hidden');
        btn.innerHTML = "Show More"

      }
    }

  </script>
</head>

<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <img src="static/images/icon.png"
              alt="Schematic showing 3 differnt types of task applicable for deep learning. (image to values, image to image & 2D to 3D)" />

            <h1 class="title is-1 publication-title">DEEP-EM TOOLBOX:</h1>
            <h2 class="title is-1 publication-title">Deep Learning Toolbox for Electron Microscopy Researchers</h2>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://viscom.uni-ulm.de/members/hannah-kniesel/" target="_blank">Hannah
                  Kniesel</a><sup>1</sup>,</span>
              <span class="author-block">
                <a href="https://viscom.uni-ulm.de/members/tristan-payer/" target="_blank">Tristan
                  Payer</a><sup>1</sup>,</span>
              <span class="author-block">
                <a href="https://viscom.uni-ulm.de/members/poonam/" target="_blank">Poonam Poonam</a><sup>1</sup>,
              </span>
              <span class="author-block">
                <a href="" target="_blank">Tim Bergner</a><sup>2</sup>,
              </span>

              <span class="author-block">
                <a href="https://phermosilla.github.io/" target="_blank">Pedro Hermosilla</a><sup>3</sup>
              </span>
              <span class="author-block">
                <a href="https://viscom.uni-ulm.de/members/timo-ropinski/" target="_blank">Timo
                  Ropinski</a><sup>1</sup>,
              </span>

            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block"><sup>1</sup>Visual Computing Group, Ulm University<br><sup>2</sup>Central
                Facility Electron Microscopy, Ulm Univesity<br><sup>3</sup>Computer Vision Lab, TU Vienna</span>
              <!-- <span class="eql-cntrb"><small><br><sup>*</sup>Indicates Equal Contribution</small></span> -->
            </div>

            <div class="column has-text-centered">
              <div class="publication-links">
                <!-- Arxiv PDF link -->
                <span class="link-block">
                  <a href="https://arxiv.org/pdf/<ARXIV PAPER ID>.pdf" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                    </span>
                    <span>Paper</span>
                  </a>
                </span>

                <!-- Supplementary PDF link 
                    <span class="link-block">
                      <a href="static/pdfs/supplementary_material.pdf" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Supplementary</span>
                    </a>
                  </span>-->

                <!-- Github link 
                  <span class="link-block">
                    <a href="https://github.com/YOUR REPO HERE" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span> -->

                <!-- ArXiv abstract Link 
                <span class="link-block">
                  <a href="https://arxiv.org/abs/<ARXIV PAPER ID>" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>-->
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>


  <!-- Teaser image
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
    <img src="static/images/tasks.png" alt="Schematic showing 3 differnt types of task applicable for deep learning. (image to values, image to image & 2D to 3D)" />
      <h2 class="subtitle has-text-centered">We propose to categorize tasks within the area of EM data analysis into Image to Value(s), Image to Image and 2D to 3D. We do so, based on their specific requirements for implementing a deep learning workflow. For more details, please see our paper.</h2>
    </div>
  </div>
</section>
End teaser image -->

  <!-- Teaser image
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
    <img src="static/images/workflow.png" alt="Standard Deep Learning Workflow" />
      <h2 class="subtitle has-text-centered">
        Figure 1: We propose a simple workflow for developing deep learning solutions for the supported analysis of EM data. 
        The workflow is clustered into three categories: 1) Task; 2) Data; 3) Model</h2>
    </div>
  </div>
</section>
End teaser image -->



  <!-- Paper abstract -->
  <section class="section hero is-light">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Abstract</h2>
          <div class="content has-text-justified">
            <p>
              Despite advancements in computer vision, deep learning application in EM labs remains limited. This paper
              outlines various application areas within EM and presents a straightforward deep learning workflow for
              developing solutions in this context.
              We aim to bridge the gap between deep learning experts and electron microscopy (EM) researchers,
              acknowledging the significant potential of deep learning in enhancing the analysis of EM micrographs. With
              its proven success in computer vision tasks, deep learning can revolutionize EM image analysis through
              supported, automated, and standardized methodologies. We introduce the Deep Learning Toolbox for Electron
              Microscopy Researchers to integrate deep learning into EM data analysis.
              Our primary objective is to foster effective collaboration between domain experts and data scientists,
              addressing differences in terminology and expertise. We also introduce a platform to compile recent
              advancements in deep learning for EM, demonstrating its capabilities through three exemplary notebooks for
              tasks such as virus quantification in EM images, segmentation, and tomographic reconstruction. The
              platform is designed for plug-and-play use by EM researchers, and we encourage contributions from the
              research community to make their work accessible.
            </p>
          </div>
        </div>
      </div>
    </div>
  </section>
  <!-- End paper abstract -->



  <!--Intro to Deep Learning -->
  <section class="section hero">
    <div class="container is-max-desktop content">
      <h2 class="title is-3">Deep Learning Terminology</h2>
      <div class="content has-text-justified">
        <p>Deep Learning has emerged as a powerful tool of artificial intelligence.
          Deep Learning describes a tool, which, in theory, is able to approximate any function \( f_{\theta}(x) =
          \hat{y} \), where \( x \) is some input data
          (like a micrograph of a virus infected cell) and \( \hat{y} \) is the network's output. During training of the
          neural network, the function's parameters \( \theta \)
          (often referred to as <i>trainable parameters</i>) need to be adjusted, such that \( \hat{y} = y \), where \(
          y \) is a desired output of the model (like the number of virus capsids
          present in the input image. \( y \) is often called "labels", "ground truth", "target" or "annotations").
          To train the network, we need to define a <i>loss function</i> \( L(\hat{y}, y) \), where \( \hat{y} =
          f_{\theta}(x) \) (the network's output), which is able to measure the network's
          error. Then, the parameters \( \theta \) of the network are updated using <i>gradient descent</i>. Using
          gradient descent, we are aiming to minimize the predefined loss function
          for a large set of training data \( x_i \in X_{i=1...N} \).
          By using a large dataset with high variances in the data we aim to make the network <i>generalizable</i>,
          which means that the network is able to learn a
          function \( f \) which is able to map input data, which it has not seen during training to a correct
          prediction.
        </p>
        <p>In the following, we give a short overview of most common terminology used in the context of deep learning.
        </p>
      </div>
      <button id="togglebtn-terminology" class="button-55"
        onclick="toggleVisibility('content-terminology', 'togglebtn-terminology')">Show More</button>
      <div id="content-terminology" class="hidden">
        <p></p>
        <p>
          <strong>Loss function</strong> is a mathematical function that quantifies the difference between the predicted
          output of a neural network and the actual target value (often also referred to as <i>annotation</i>, <i>ground
            truth</i> or <i>label</i>). It serves as a crucial component in training deep learning models by providing a
          measure of how well or poorly the model is performing. The primary objective during training is to minimize
          this loss function, which in turn improves the model's predictions.
        </p>
        <p>
          <strong>Metric</strong> In the context of deep learning, a metric is a quantitative measure used to evaluate
          the performance of a model. Metrics provide insights into how well the model is performing on tasks such as
          classification, regression, or other predictive tasks by comparing the model's predictions to the actual
          ground truth values. Metrics help in assessing the effectiveness of the model, guiding the tuning of
          hyperparameters, and making decisions about model improvements. Unlike loss functions, which are optimized
          during training, metrics are primarily used for evaluation purposes, providing a clearer understanding of the
          model's predictive capabilities and generalization to unseen data.
        </p>
        <p>
          <strong>Gradient Descent</strong> Gradient descent is a fundamental optimization algorithm used in deep
          learning to minimize the loss function. The algorithm iteratively adjusts the trainable parameters (weights
          and biases) of the neural network to reduce this loss. The core idea involves computing the gradient (partial
          derivative) of the loss function with respect to each parameter. These gradients indicate the direction and
          rate of change needed to decrease the loss. The parameters are then updated in the opposite direction of the
          gradient, scaled by a learning rate, which controls the step size of the updates. Mathematically, the update
          rule for a parameter θ at update step t is given by θ<sub>t</sub> = θ<sub>t-1</sub> - η∇<sub>θ</sub>L, where η
          is the learning rate, ∇<sub>θ</sub>L is the gradient of the loss L with respect to θ. This iterative process
          continues until the algorithm converges to a minimum of the loss function, ideally reaching optimal parameter
          values that allow the neural network to make accurate predictions. Gradient descent variants, such as
          stochastic gradient descent (SGD) and mini-batch gradient descent, improve efficiency and performance by
          adjusting how the gradients are computed and applied.
        </p>
        <p>
          <strong>Architectures</strong> refer to the specific design and configuration of neural networks, dictating
          how layers are arranged and interconnected. Common architectures include Convolutional Neural Networks (CNNs)
          for image processing, Recurrent Neural Networks (RNNs) for sequential data, and Transformer models for tasks
          like natural language processing or image processing. Each architecture is tailored to handle specific types
          of input and output dimensions, ensuring optimal processing and learning.
          At the core of these architectures are neurons, the fundamental units of a neural network. A neuron receives
          input, processes it using a set of weights, and then applies an activation function, such as ReLU (Rectified
          Linear Unit), Sigmoid, or Tanh, to introduce non-linearity, enabling the network to learn complex functions.
          Layers, which are collections of neurons, form the structural components of a neural network. There are
          various types of layers, each serving a distinct purpose. For example, input layers handle the raw data,
          hidden layers process the input through multiple transformations, and output layers produce the final
          predictions. The architecture must also adapt the input dimensions, like the dimension of the input data, and
          the output dimensions, for example to ensure the correct number of classes in classification tasks, to suit
          the problem being addressed. The thoughtful design of these architectures, the role of neurons, the
          appropriate activation functions, and the strategic use of different types of layers are essential for the
          network to effectively learn from the data and perform the desired tasks.
        </p>
        <p>
          <strong>Hyperparameters</strong> in the context of deep learning are the parameters set before the training
          process begins, which govern the overall behavior and performance of the neural network. Unlike model
          parameters, which are learned during training, hyperparameters need to be manually defined. They include
          aspects such as the learning rate, batch size, number of epochs, and architecture-specific choices like the
          number of layers and units per layer. The choice of hyperparameters can significantly impact the model's
          ability to learn effectively and generalize to new data. Tuning these hyperparameters is often a complex and
          iterative process, involving techniques such as grid search, random search, or more sophisticated methods like
          Bayesian optimization to find the optimal settings that enhance model performance.
        </p>
        <p>
          <strong>Training</strong> in deep learning is the process where a neural network learns from a dataset by
          adjusting its weights to minimize the error of its predictions. The dataset is often too large to process all
          at once, so it is divided into smaller subsets called batches. A batch is a small, manageable portion of the
          dataset used to update the model's weights. Training on batches is necessary because it allows for efficient
          computation and memory usage, making it feasible to train large models on large datasets.
          An iteration refers to a single update of the model's weights using one batch of data. Multiple iterations
          make up an epoch, which is a complete pass through the entire training dataset. Training on batches helps
          achieve a balance between speed and accuracy, as each batch update can quickly provide feedback to the model,
          allowing it to adjust its weights incrementally.
          Using a batch size of 1, also known as online learning, can be inefficient and noisy. With a batch size of 1,
          the model's weights are updated after every single data point, leading to highly variable gradient updates
          that can make the training process unstable and slow. Larger batch sizes help in smoothing out these updates,
          providing more stable and reliable gradients, which can lead to more efficient convergence.
          Throughout many epochs, the model iteratively processes batches of data, computes predictions, and updates its
          parameters using optimization algorithms such as stochastic gradient descent. The goal is to minimize a
          predefined loss function that quantifies the discrepancy between the predicted outputs and the actual targets.
          By iteratively refining its weights through batch processing, the model learns the underlying patterns in the
          data effectively, leading to improved performance and generalization.
        </p>
        <p>
          <strong>Learning Rate</strong> The learning rate is a critical hyperparameter that determines the step size at
          each iteration while moving towards a minimum of the loss function. A learning rate that is too high can cause
          the training process to converge too quickly to a suboptimal solution, or even diverge. Conversely, a learning
          rate that is too low can make the training process very slow, potentially getting stuck in local minima.
        </p>
        <p>
          <strong>Learning Rate Scheduler</strong> To address the challenges of selecting a proper learning rate,
          learning rate schedulers are used. These dynamically adjust the learning rate during training to improve
          performance and convergence speed. Common strategies include:
        <ul>
          <li><i>Step Decay</i>: Reduces the learning rate by a factor at fixed intervals (epochs).</li>
          <li><i>Exponential Decay</i>: Gradually decreases the learning rate exponentially over time.</li>
          <li><i>Cosine Annealing</i>: Reduces the learning rate following a cosine curve, which can help in exploring
            wider regions of the loss landscape initially and then fine-tuning as training progresses.</li>
          <li><i>Cyclic Learning Rate</i>: Varies the learning rate cyclically between a minimum and maximum boundary,
            which can help escape local minima and improve training performance.</li>
        </ul>
        </p>
        <p>
          <strong>Optimization Algorithms</strong> Optimization algorithms are used to adjust the weights of the model
          to minimize the loss function. Different optimizers offer various advantages depending on the problem and the
          dataset. Here are some commonly used optimizers:
        <ul>
          <li><i>Stochastic Gradient Descent (SGD)</i>: SGD updates the model's parameters using the gradient of the
            loss function with respect to the parameters for each batch of data. It is simple and effective but can be
            slow to converge and may oscillate near the minimum.</li>
          <li><i>Momentum</i>: An extension of SGD, momentum helps accelerate SGD by navigating in the relevant
            direction and dampening oscillations. It accumulates a velocity vector in the direction of the gradient's
            consistent component, speeding up the training process.</li>
          <li><i>Adagrad</i>: Adagrad adapts the learning rate for each parameter based on its gradients' historical
            sum. It is particularly useful for dealing with sparse data but can suffer from decaying learning rates over
            time.</li>
          <li><i>RMSprop</i>: RMSprop adjusts the learning rate for each parameter by dividing by a running average of
            recent gradients' magnitudes. It mitigates Adagrad's issue of decaying learning rates and performs well in
            practice.</li>
          <li><i>Adam</i>: Adam (Adaptive Moment Estimation) combines the benefits of both Adagrad and RMSprop. It
            computes adaptive learning rates for each parameter using the first and second moments of the gradients.
            Adam is widely used due to its robust performance across various tasks.</li>
          <li><i>AdamW</i>: An extension of Adam, AdamW decouples weight decay (used for regularization) from the
            gradient updates. This improves the optimizer's performance, particularly when using L2 regularization.</li>

        </ul>
        </p>
        <p>
          <strong>Batch Size</strong> The batch size is a crucial hyperparameter in deep learning training that defines
          the number of samples processed before the model's internal parameters are updated. It influences both the
          learning dynamics and computational efficiency of the training process. Choosing the right batch size involves
          balancing several trade-offs. Smaller batch sizes (e.g., 32 or 64) provide more frequent updates to the model
          parameters, which can lead to a smoother convergence and better generalization to new data. However, they may
          introduce higher noise in the gradient estimates, which can make the training process less stable. Larger
          batch sizes (e.g., 256 or 512) offer more accurate gradient estimates and can leverage parallel processing
          capabilities of modern GPUs more efficiently, potentially speeding up the training process. Yet, they require
          more memory and can lead to less frequent updates, which might result in slower convergence and risk of
          getting stuck in local minima. Empirically, a batch size that balances these factors is typically chosen based
          on the specific dataset and computational resources available. Adaptive strategies, such as progressively
          increasing the batch size during training, can also be employed to combine the benefits of both small and
          large batch sizes.
        </p>
        <p>
          <strong>Validation</strong> is a critical step in deep learning used to evaluate the model's performance on a
          separate dataset not seen during training. This dataset, called the validation set, is used to tune
          hyperparameters, select the best model, and prevent overfitting. Overfitting occurs when a model learns the
          training data too well, capturing noise and specific patterns that do not generalize to new, unseen data. This
          leads to poor performance on validation or test sets. In contrast, generalization is the model's ability to
          perform well on new, unseen data, indicating that it has learned the underlying patterns in the training data
          without memorizing it. During training, the model's performance on the validation set is monitored, and
          adjustments are made to improve generalization. This helps ensure that the model does not just memorize the
          training data but learns to generalize to new, unseen data, enhancing its robustness and applicability in
          real-world scenarios.
        </p>
        <p>
          <strong>Test</strong> The test phase, sometimes also refered to as inference, is where the trained model is
          evaluated on a completely separate dataset called the test set. This dataset is used to assess the model's
          final performance and its ability to generalize to new data. During inference, the model makes predictions on
          new data points, and its performance metrics (such as accuracy, precision, recall) are calculated. This phase
          is crucial for understanding how well the model will perform in real-world scenarios and ensures that the
          model's performance is robust and reliable.
        </p>
        <p>
          <strong>Supervised Learning</strong> is the standard approach of machine learning where the model is trained
          on labeled data. In this paradigm, the training dataset consists of input-output pairs, where each input $x$
          is associated with a known output $y$ (label). The goal of supervised learning is to learn a mapping function
          from inputs to outputs, allowing the model to make accurate predictions on new, unseen data. Supervised
          learning is widely used in various domains such as image recognition, speech recognition, and medical
          diagnosis, due to its effectiveness in learning from explicit examples.
        </p>
        <p>
          <strong>Weakly Supervised Learning</strong> is a machine learning approach where the model is trained using
          partially labeled or noisy data, as opposed to fully labeled data in traditional supervised learning. In
          weakly supervised learning, the training dataset may contain only high-level labels, partial labels, or noisy
          labels, which provide limited or ambiguous information about the ground truth. Despite the challenges posed by
          the lack of precise labels, weakly supervised learning algorithms aim to infer meaningful patterns and
          relationships from the available data to make predictions or perform tasks. This approach often requires
          innovative techniques, such as label aggregation, data augmentation, or learning from indirect supervision
          signals. Weakly supervised learning is particularly useful in scenarios where obtaining fully labeled data is
          expensive, time-consuming, or impractical, allowing models to be trained on larger, more diverse datasets.
          Additionally, weak supervision can function as implicit standardization: For example when human opinion on the
          full annotation is ambiguous, the annotations in a weak scenario might be unambiguous. Hence, the model is
          able to learn a standardization from the unambiguous weak labels.
        </p>
        <p>
          <strong>Unsupervised Learning</strong> is particularly valuable for pretraining models on large, unlabeled
          datasets. Instead of relying on labeled examples, unsupervised learning algorithms explore the raw input data
          to extract meaningful features or representations without explicit guidance. Pretraining involves training a
          model on a large amount of unlabeled data to learn general patterns and structures in the data. Once
          pretrained, the model can be fine-tuned on smaller labeled datasets for specific tasks, such as classification
          or regression. Fine-tuning adjusts the pretrained model's parameters to make it better suited for the specific
          task at hand, leveraging the knowledge gained during pretraining. Unsupervised pretraining followed by
          fine-tuning has proven to be effective in improving model performance, especially in scenarios where labeled
          data is scarce or expensive to obtain. It plays a crucial role in various applications such as natural
          language processing, computer vision, and speech recognition, enabling the development of more accurate and
          robust deep learning models.
        </p>
      </div>
    </div>
  </section>
  <!--End intro to Deep Learning  -->



  <!--Workflow -->
  <section class="section hero is-light">
    <div class="container is-max-desktop content">
      <h2 class="title is-3">Workflow</h2>
      <div class="hero-body">
        <img src="static/images/workflow.png" alt="deep learning workflow in the context of EM." />
        <h6 class="subtitle has-text-centered">
          Figure 1: We propose a simple workflow for developing deep learning solutions for the supported analysis of EM
          data.
          The workflow is clustered into three categories: 1) Task; 2) Data; 3) Model </h6>
      </div>
      <div class="content has-text-justified">
        <p>The introduced DEEP-EM TOOLBOX follows a generalizable workflow, which we introduce in the
          following.
          In our workflow (Figure 1) we propose 3 clusters:</p>
        <ul>
          <li>Task (orange).</li>
          <li>Data (green).</li>
          <li>Model (red).</li>
        </ul>

        <p>
          The standardized workflow allows easier access and realization of adaptions to the methods.
          Additionally, we identify and analyse possible challenges with applying DL to EM data and
          discuss how to tackle them.


        </p>
        <p></p>

        <div class="orange-background">
          <h3>Task</h3>
          <p>
            For developing DL solutions for complex datasets, such as EM images, it is essential to grasp both the
            inherent characteristics of the data and the specific tasks that need to be addressed. This section will
            outline the necessary steps for defining tasks and architectures, providing a comprehensive foundation
            for effectively applying deep learning techniques to EM image analysis.
          </p>

          <button id="togglebtn-task-model" class="button-55"
            onclick="toggleVisibility('content-task-model', 'togglebtn-task-model')">Show More</button>
          <div id="content-task-model" class="hidden">
            <p></p>
            <h4>Definition</h4>
            <h4>Architecture</h4>
          </div>

        </div>
        <div class="green-background">
          <h3>Data</h3>
          <p>
            Even though it is well known that large dataset sizes can drastically improve the performance of DL
            models, more data does not always equate to better model performance. High-quality data is essential
            for deriving meaningful correlations between inputs and outputs, providing a strong learning signal.
            Models must also be robust against artifacts in EM data and generalize across different sampling
            methods, detectors, and microscopes. Ensuring balanced occurrence and variance within datasets is
            crucial. Achieving optimal model performance requires balancing data quality, variance, robustness,
            and dataset size. This is vital because a trained model functions as a black box, making it difficult to
            correct biases or errors later.
          </p>
          <button id="togglebtn-data" class="button-55"
            onclick="toggleVisibility('content-data', 'togglebtn-data')">Show More</button>
          <div id="content-data" class="hidden">
            <p></p>
            <h4>Acquisition</h4>
            <h4>Annotation</h4>
            <h4>Preprocessing</h4>
          </div>

        </div>
        <div class="red-background">
          <h3> Model</h3>
          <p>
            Model training is the process of teaching a DL model to recognize patterns in data by adjusting its
            parameters to minimize prediction errors. Model evaluation assesses the trained model’s performance
            on unseen data to determine its effectiveness and generalization capabilities, ensuring that it can
            accurately predict outcomes in real-world scenarios.
          </p>
          <button id="togglebtn-model" class="button-55"
            onclick="toggleVisibility('content-model', 'togglebtn-model')">Show More</button>
          <div id="content-model" class="hidden">
            <p></p>
            <h4>Training</h4>
            <h4>Evaluation</h4>
          </div>

        </div>
      </div>
    </div>
  </section>
  <!--End workflow  -->



  <!--Links -->
  <section class="section hero">
    <div class="container is-max-desktop content">
      <h2 class="title is-3">Use Cases</h2>
      <div class="hero-body">
        <img src="static/images/tasks.png" alt="Categories of deep learning tasks in the context of EM." />
        <h6 class="subtitle has-text-centered">Tasks in the Area of EM data analysis can be categorized by the
          requirements of the DL
          method into <strong>Image to Value(s)</strong>, <strong>Image to Image</strong> and <strong>2D to 3D</strong>.
          For each category, we introduce one exemplary notebooks, tackling EM specific challenges. </h6>
      </div>
      <div class="content has-text-justified">
        <p>For providing exemplary notebooks, we are using lighting studio, as they are easy to setup, access and work
          with. A quick introduction can be found here: (https://lightning.ai/docs/overview/studios)</p>
        <p>We design our notebooks in such way, that they follow the suggested DEEP-EM TOOLBOX workflow stages.
          Within our notebooks, we further provide additional info on how to annotate and preprocess your own data in
          order to plug it into the pipeline of the use case.</p>
        <h3>Image to Value(s)</h3>
        <h4>Explainable Virus Quantification</h4>
        <h5>Challenge: Deep Learning as Black Box</h5>
        <p>Within this Image to Value(s) task, we are developing a regression model to quantify virus capsids and their
          mutation stages ("naked", "budding", "enveloped") during secondary envelopment in TEM images.
          We encourage researchers to adapt this notebook to use their own data to simplify the analysis of EM images in
          their own lab.
          For further details please see the project page.
        </p>
        <button class="button-55"
          onclick="location.href='https://viscom-ulm.github.io/DeepEM/explainable-virus-quantification';">
          Go to Project
        </button>
        <figure>
          <img src="static/images/explainable-virus-quantification/Teaser.png"
            alt="Teaser explainable virus quantification">
          <figcaption id="fig:teaser-ensemble">
            For the explainable virus quantification we train a regression model to predict the number of "naked",
            "budding" and "enveloped" virus capsids in the input image.
            We use GradCAM as an explainable AI technique, to make the model more trustworthy and the predictions
            easier to coprehend.
          </figcaption>
        </figure>


        <hr>


        <h3>Image to Image</h3>
        <h4>Segmentation of Cellular Structures</h4>
        <h5>Challenge: Robustness with small dataset sizes</h5>

        <p>We choose the segmentation of certain cell organelles as a relevant Image to Image task.
          Segmentation is an important tool in EM image analysis as it contributes to a better visualisation of
          certain
          organelles and complex cell structures,
          which facilitates the interpretation of EM data. Segmentation allows for detailed analysis of organelle
          morphology, spatial relationships and distribution within cells.
          This is crucial for understanding intracellular organisation and its relationship to cell function.
          Due to the small available dataset sizes we deploy data augmentation methods, make use of pretrained weights
          and train an ensemble model, which has been shown to provide
          better generalizability even when trained on smaller dataset sizes [1].
          For further details please see the project page.
        </p>
        <button class="button-55"
          onclick="location.href='https://viscom-ulm.github.io/DeepEM/image-to-value/segmentation-of-cellular-structures';">
          Go to Project
        </button>
        <figure>
          <img src="static/images/segmentation-of-cellular-structures/Teaser-ensemble.png"
            alt="Depiction of ensemble model">
          <figcaption id="fig:teaser-ensemble">
            For the segmentation of cellular structures we follow [1] and train a so called "ensemble" model.
            An ensemble model is a set of models which are used to make multiple predictions for the same input data.
            The predictions are then combined to retrieve a more robust prediction.

          </figcaption>
        </figure>
        <p><i>[1] Shaga Devan, Kavitha, et al. "Weighted average ensemble-based semantic segmentation in biological
            electron microscopy images." Histochemistry and Cell Biology 158.5 (2022): 447-462.</i></p>


        <hr>

        <h3>2D to 3D</h3>
        <h4>Deep Learning Based Tomographic Reconstruction of Scanning Transmission Electron Microscopy (STEM) Images
        </h4>
        <h5>Challenge: Evaluation with missing ground truth</h5>
        <p>To introduce 2D to 3D tasks, we implement a learning based tomographic reconstruction of 2D projections
          obtained from STEM tomograms, following [1,2].
          Reconstruction of a 3D volume allows visualisation of the true morphology, spatial relationships and
          connectivity of certain cellular structures and
          organelles within a cell, which may not be visible in 2D projections alone. Due to missing ground truth
          information in the case of tomographic reconstruction,
          we make use of pre-existing synthetic data to assess the model performance.
          For further details please see the project page.
        </p>
        <button class="button-55"
          onclick="location.href='https://viscom-ulm.github.io/DeepEM/image-to-value/tomographic-reconstruction-stem';">
          Go to Project
        </button>
        <figure>
          <img src="static/images/tomographic-reconstruction-stem/Teaser.png"
            alt="Depiction of the tomographic reconstruction. (Show tilt series, model, reconstruction of Nanoparticles)">
          <figcaption id="fig:Tomo">
            Based on a given tilt series, we are able to generate a 3D reconstruction using self-supervised deep learning.
          </figcaption>
        </figure>


        <p></p>
        <p><i>[1] Kniesel, Hannah, et al. "Clean implicit 3d structure from noisy 2d stem images." Proceedings of the
            IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2022.</i></p>
        <p><i>[2] Mildenhall, Ben, et al. "Nerf: Representing scenes as neural radiance fields for view synthesis."
            Communications of the ACM 65.1 (2021): 99-106.</i></p>

        <hr>

      </div>

    </div>
  </section>
  <!--End Links  -->

  <!--BibTex citation -->
  <section class="section hero">
    <div class="container is-max-desktop content">
      <h2 class="title">Your Turn!</h2>
      <p>
        We further encourage contributions from the research community.
        We ask you to test and use the provided notebooks within your own lab and adapt them to your needs.
        We appreciate any further contributions of use cases to make them easily accessible to the EM research
        community.


      </p>
      <p>Please don't hesitate to contact us!</p>
    </div>
  </section>
  <!--End BibTex citation -->



  <!--BibTex citation -->
  <section class="section hero" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>BibTex Code Here</code></pre>
    </div>
  </section>
  <!--End BibTex citation -->

  <!-- Footer  -->
  <footer class="footer">
    <div class="container">
      <div class="columns is-centered">
        <div class="column is-8">
          <div class="content">

            <p>
              This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template"
                target="_blank">Academic Project Page Template</a> which was adopted from the <a
                href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
              You are free to borrow the of this website, we just ask that you link back to this page in the footer.
              <br> This website is licensed under a <a rel="license"
                href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
                Commons Attribution-ShareAlike 4.0 International License</a>.
            </p>

          </div>
        </div>
      </div>
    </div>
  </footer>
  <!-- End footer -->

  <!-- Statcounter tracking code -->

  <!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

  <!-- End of Statcounter Code -->

</body>

</html>