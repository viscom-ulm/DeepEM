<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="DEEP-EM TOOLBOX" />
  <meta property="og:description"
    content="Unlock the power of Deep Learning in Electron Microscopy with the DEEP-EM TOOLBOX standardized workflows for EM image analysis." />
  <meta property="og:url" content="URL OF THE WEBSITE" />
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200" />
  <meta property="og:image:height" content="630" />


  <meta name="twitter:title" content="DEEP-EM TOOLBOX">
  <meta name="twitter:description"
    content="Unlock the power of Deep Learning in Electron Microscopy with the DEEP-EM TOOLBOX standardized workflows for EM image analysis.">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="Deep Learning, Electron Microscopy, Data Analysis, Data Interpretation, Toolbox">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>DEEP-EM TOOLBOX</title>
  <link rel="icon" type="image/x-icon" href="static/images/icon.png">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">
  <style>
    .hidden {
      display: none;
    }

    .button-55 {
      align-self: center;
      background-color: #fff;
      background-image: none;
      background-position: 0 90%;
      background-repeat: repeat no-repeat;
      background-size: 4px 3px;
      border-radius: 15px 225px 255px 15px 15px 255px 225px 15px;
      border-style: solid;
      border-width: 2px;
      box-shadow: rgba(0, 0, 0, .2) 15px 28px 25px -18px;
      box-sizing: border-box;
      color: #41403e;
      cursor: pointer;
      display: inline-block;
      font-family: Neucha, sans-serif;
      font-size: 1rem;
      line-height: 23px;
      outline: none;
      padding: .75rem;
      text-decoration: none;
      transition: all 235ms ease-in-out;
      border-bottom-left-radius: 15px 255px;
      border-bottom-right-radius: 225px 15px;
      border-top-left-radius: 255px 15px;
      border-top-right-radius: 15px 225px;
      user-select: none;
      -webkit-user-select: none;
      touch-action: manipulation;
    }

    .button-55:hover {
      box-shadow: rgba(0, 0, 0, .3) 2px 8px 8px -5px;
      transform: translate3d(0, 2px, 0);
    }

    .button-55:focus {
      box-shadow: rgba(0, 0, 0, .3) 2px 8px 4px -6px;
    }

    .green-background {
      background-color: #dde9afff;
      /* Green background */
      padding: 20px;
      /* Padding inside the element */
      border-radius: 10px;
      /* Rounded corners */
      box-shadow: 0 4px 8px rgba(0, 0, 0, 0.2);
      /* Subtle shadow for depth */
      font-size: 16px;
      /* Font size */
      margin: 20px 0;
      /* Margin outside the element */
    }

    .red-background {
      background-color: #ffaaaaff;
      /* Green background */
      padding: 20px;
      /* Padding inside the element */
      border-radius: 10px;
      /* Rounded corners */
      box-shadow: 0 4px 8px rgba(0, 0, 0, 0.2);
      /* Subtle shadow for depth */
      font-size: 16px;
      /* Font size */
      margin: 20px 0;
      /* Margin outside the element */
    }

    .orange-background {
      background-color: #ffb380ff;
      /* Green background */
      padding: 20px;
      /* Padding inside the element */
      border-radius: 10px;
      /* Rounded corners */
      box-shadow: 0 4px 8px rgba(0, 0, 0, 0.2);
      /* Subtle shadow for depth */
      font-size: 16px;
      /* Font size */
      margin: 20px 0;
      /* Margin outside the element */
    }
  </style>

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  <script>
    function toggleVisibility(id_content, id_button) {
      var content = document.getElementById(id_content);
      var btn = document.getElementById(id_button);


      if (content.classList.contains('hidden')) {
        content.classList.remove('hidden');
        btn.innerHTML = "Show Less"
      } else {
        content.classList.add('hidden');
        btn.innerHTML = "Show More"

      }
    }

  </script>
</head>

<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <img src="static/images/icon.png"
              alt="Schematic showing 3 differnt types of task applicable for deep learning. (image to values, image to image & 2D to 3D)" />

            <h1 class="title is-1 publication-title">DEEP-EM TOOLBOX:</h1>
            <h2 class="title is-1 publication-title">Deep Learning Toolbox for Electron Microscopy Researchers</h2>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://viscom.uni-ulm.de/members/hannah-kniesel/" target="_blank">Hannah
                  Kniesel</a><sup>1</sup>,</span>
              <span class="author-block">
                <a href="https://viscom.uni-ulm.de/members/tristan-payer/" target="_blank">Tristan
                  Payer</a><sup>1</sup>,</span>
              <span class="author-block">
                <a href="https://viscom.uni-ulm.de/members/poonam/" target="_blank">Poonam Poonam</a><sup>1</sup>,
              </span>
              <span class="author-block">
                <a href="" target="_blank">Tim Bergner</a><sup>2</sup>,
              </span>

              <span class="author-block">
                <a href="https://phermosilla.github.io/" target="_blank">Pedro Hermosilla</a><sup>3</sup>
              </span>
              <span class="author-block">
                <a href="https://viscom.uni-ulm.de/members/timo-ropinski/" target="_blank">Timo
                  Ropinski</a><sup>1</sup>,
              </span>

            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block"><sup>1</sup>Visual Computing Group, Ulm University<br><sup>2</sup>Central
                Facility Electron Microscopy, Ulm Univesity<br><sup>3</sup>Computer Vision Lab, TU Vienna</span>
              <!-- <span class="eql-cntrb"><small><br><sup>*</sup>Indicates Equal Contribution</small></span> -->
            </div>

            <div class="column has-text-centered">
              <div class="publication-links">
                <!-- Arxiv PDF link -->
                <span class="link-block">
                  <a href="https://arxiv.org/pdf/<ARXIV PAPER ID>.pdf" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                    </span>
                    <span>Paper</span>
                  </a>
                </span>

                <!-- Supplementary PDF link 
                    <span class="link-block">
                      <a href="static/pdfs/supplementary_material.pdf" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Supplementary</span>
                    </a>
                  </span>-->

                <!-- Github link 
                  <span class="link-block">
                    <a href="https://github.com/YOUR REPO HERE" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span> -->

                <!-- ArXiv abstract Link 
                <span class="link-block">
                  <a href="https://arxiv.org/abs/<ARXIV PAPER ID>" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>-->
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>


  <!-- Teaser image
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
    <img src="static/images/tasks.png" alt="Schematic showing 3 differnt types of task applicable for deep learning. (image to values, image to image & 2D to 3D)" />
      <h2 class="subtitle has-text-centered">We propose to categorize tasks within the area of EM data analysis into Image to Value(s), Image to Image and 2D to 3D. We do so, based on their specific requirements for implementing a deep learning workflow. For more details, please see our paper.</h2>
    </div>
  </div>
</section>
End teaser image -->

  <!-- Teaser image
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
    <img src="static/images/workflow.png" alt="Standard Deep Learning Workflow" />
      <h2 class="subtitle has-text-centered">
        Figure 1: We propose a simple workflow for developing deep learning solutions for the supported analysis of EM data. 
        The workflow is clustered into three categories: 1) Task; 2) Data; 3) Model</h2>
    </div>
  </div>
</section>
End teaser image -->



  <!-- Paper abstract -->
  <section class="section hero is-light">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Abstract</h2>
          <div class="content has-text-justified">
            <p>
              Despite advancements in computer vision, deep learning application in EM labs remains limited. This paper
              outlines various application areas within EM and presents a straightforward deep learning workflow for
              developing solutions in this context.
              We aim to bridge the gap between deep learning experts and electron microscopy (EM) researchers,
              acknowledging the significant potential of deep learning in enhancing the analysis of EM micrographs. With
              its proven success in computer vision tasks, deep learning can revolutionize EM image analysis through
              supported, automated, and standardized methodologies. We introduce the Deep Learning Toolbox for Electron
              Microscopy Researchers to integrate deep learning into EM data analysis.
              Our primary objective is to foster effective collaboration between domain experts and data scientists,
              addressing differences in terminology and expertise. We also introduce a platform to compile recent
              advancements in deep learning for EM, demonstrating its capabilities through three exemplary notebooks for
              tasks such as virus quantification in EM images, segmentation, and tomographic reconstruction. The
              platform is designed for plug-and-play use by EM researchers, and we encourage contributions from the
              research community to make their work accessible.
            </p>
          </div>
        </div>
      </div>
    </div>
  </section>
  <!-- End paper abstract -->



  <!--Intro to Deep Learning -->
  <section class="section hero">
    <div class="container is-max-desktop content">
      <h2 class="title is-3">Deep Learning Terminology</h2>
      <div class="content has-text-justified">
        <p>Deep Learning has emerged as a powerful tool of artificial intelligence.
          Deep Learning describes a tool, which, in theory, is able to approximate any function \( f_{\theta}(x) =
          \hat{y} \), where \( x \) is some input data
          (like a micrograph of a virus infected cell) and \( \hat{y} \) is the network's output. During training of the
          neural network, the function's parameters \( \theta \)
          (often referred to as <i>trainable parameters</i>) need to be adjusted, such that \( \hat{y} = y \), where \(
          y \) is a desired output of the model (like the number of virus capsids
          present in the input image. \( y \) is often called "labels", "ground truth", "target" or "annotations").
          To train the network, we need to define a <i>loss function</i> \( L(\hat{y}, y) \), where \( \hat{y} =
          f_{\theta}(x) \) (the network's output), which is able to measure the network's
          error. Then, the parameters \( \theta \) of the network are updated using <i>gradient descent</i>. Using
          gradient descent, we are aiming to minimize the predefined loss function
          for a large set of training data \( x_i \in X_{i=1...N} \).
          By using a large dataset with high variances in the data we aim to make the network <i>generalizable</i>,
          which means that the network is able to learn a
          function \( f \) which is able to map input data, which it has not seen during training to a correct
          prediction.
        </p>
        <p>In the following, we give a short overview of most common terminology used in the context of deep learning.
        </p>
      </div>
      <button id="togglebtn-terminology" class="button-55"
        onclick="toggleVisibility('content-terminology', 'togglebtn-terminology')">Show More</button>
      <div id="content-terminology" class="hidden">
        <p></p>
        <p>
          <strong>Loss function</strong> is a mathematical function that quantifies the difference between the predicted
          output of a neural network and the actual target value (often also referred to as <i>annotation</i>, <i>ground
            truth</i> or <i>label</i>). It serves as a crucial component in training deep learning models by providing a
          measure of how well or poorly the model is performing. The primary objective during training is to minimize
          this loss function, which in turn improves the model's predictions.
        </p>
        <p>
          <strong>Metric</strong> In the context of deep learning, a metric is a quantitative measure used to evaluate
          the performance of a model. Metrics provide insights into how well the model is performing on tasks such as
          classification, regression, or other predictive tasks by comparing the model's predictions to the actual
          ground truth values. Metrics help in assessing the effectiveness of the model, guiding the tuning of
          hyperparameters, and making decisions about model improvements. Unlike loss functions, which are optimized
          during training, metrics are primarily used for evaluation purposes, providing a clearer understanding of the
          model's predictive capabilities and generalization to unseen data.
        </p>
        <p>
          <strong>Gradient Descent</strong> Gradient descent is a fundamental optimization algorithm used in deep
          learning to minimize the loss function. The algorithm iteratively adjusts the trainable parameters (weights
          and biases) of the neural network to reduce this loss. The core idea involves computing the gradient (partial
          derivative) of the loss function with respect to each parameter. These gradients indicate the direction and
          rate of change needed to decrease the loss. The parameters are then updated in the opposite direction of the
          gradient, scaled by a learning rate, which controls the step size of the updates. Mathematically, the update
          rule for a parameter θ at update step t is given by θ<sub>t</sub> = θ<sub>t-1</sub> - η∇<sub>θ</sub>L, where η
          is the learning rate, ∇<sub>θ</sub>L is the gradient of the loss L with respect to θ. This iterative process
          continues until the algorithm converges to a minimum of the loss function, ideally reaching optimal parameter
          values that allow the neural network to make accurate predictions. Gradient descent variants, such as
          stochastic gradient descent (SGD) and mini-batch gradient descent, improve efficiency and performance by
          adjusting how the gradients are computed and applied.
        </p>
        <p>
          <strong>Architectures</strong> refer to the specific design and configuration of neural networks, dictating
          how layers are arranged and interconnected. Common architectures include Convolutional Neural Networks (CNNs)
          for image processing, Recurrent Neural Networks (RNNs) for sequential data, and Transformer models for tasks
          like natural language processing or image processing. Each architecture is tailored to handle specific types
          of input and output dimensions, ensuring optimal processing and learning.
          At the core of these architectures are neurons, the fundamental units of a neural network. A neuron receives
          input, processes it using a set of weights, and then applies an activation function, such as ReLU (Rectified
          Linear Unit), Sigmoid, or Tanh, to introduce non-linearity, enabling the network to learn complex functions.
          Layers, which are collections of neurons, form the structural components of a neural network. There are
          various types of layers, each serving a distinct purpose. For example, input layers handle the raw data,
          hidden layers process the input through multiple transformations, and output layers produce the final
          predictions. The architecture must also adapt the input dimensions, like the dimension of the input data, and
          the output dimensions, for example to ensure the correct number of classes in classification tasks, to suit
          the problem being addressed. The thoughtful design of these architectures, the role of neurons, the
          appropriate activation functions, and the strategic use of different types of layers are essential for the
          network to effectively learn from the data and perform the desired tasks.
        </p>
        <p>
          <strong>Hyperparameters</strong> in the context of deep learning are the parameters set before the training
          process begins, which govern the overall behavior and performance of the neural network. Unlike model
          parameters, which are learned during training, hyperparameters need to be manually defined. They include
          aspects such as the learning rate, batch size, number of epochs, and architecture-specific choices like the
          number of layers and units per layer. The choice of hyperparameters can significantly impact the model's
          ability to learn effectively and generalize to new data. Tuning these hyperparameters is often a complex and
          iterative process, involving techniques such as grid search, random search, or more sophisticated methods like
          Bayesian optimization to find the optimal settings that enhance model performance.
        </p>
        <p>
          <strong>Training</strong> in deep learning is the process where a neural network learns from a dataset by
          adjusting its weights to minimize the error of its predictions. The dataset is often too large to process all
          at once, so it is divided into smaller subsets called batches. A batch is a small, manageable portion of the
          dataset used to update the model's weights. Training on batches is necessary because it allows for efficient
          computation and memory usage, making it feasible to train large models on large datasets.
          An iteration refers to a single update of the model's weights using one batch of data. Multiple iterations
          make up an epoch, which is a complete pass through the entire training dataset. Training on batches helps
          achieve a balance between speed and accuracy, as each batch update can quickly provide feedback to the model,
          allowing it to adjust its weights incrementally.
          Using a batch size of 1, also known as online learning, can be inefficient and noisy. With a batch size of 1,
          the model's weights are updated after every single data point, leading to highly variable gradient updates
          that can make the training process unstable and slow. Larger batch sizes help in smoothing out these updates,
          providing more stable and reliable gradients, which can lead to more efficient convergence.
          Throughout many epochs, the model iteratively processes batches of data, computes predictions, and updates its
          parameters using optimization algorithms such as stochastic gradient descent. The goal is to minimize a
          predefined loss function that quantifies the discrepancy between the predicted outputs and the actual targets.
          By iteratively refining its weights through batch processing, the model learns the underlying patterns in the
          data effectively, leading to improved performance and generalization.
        </p>
        <p>
          <strong>Learning Rate</strong> The learning rate is a critical hyperparameter that determines the step size at
          each iteration while moving towards a minimum of the loss function. A learning rate that is too high can cause
          the training process to converge too quickly to a suboptimal solution, or even diverge. Conversely, a learning
          rate that is too low can make the training process very slow, potentially getting stuck in local minima.
        </p>
        <p>
          <strong>Learning Rate Scheduler</strong> To address the challenges of selecting a proper learning rate,
          learning rate schedulers are used. These dynamically adjust the learning rate during training to improve
          performance and convergence speed. Common strategies include:
        <ul>
          <li><i>Step Decay</i>: Reduces the learning rate by a factor at fixed intervals (epochs).</li>
          <li><i>Exponential Decay</i>: Gradually decreases the learning rate exponentially over time.</li>
          <li><i>Cosine Annealing</i>: Reduces the learning rate following a cosine curve, which can help in exploring
            wider regions of the loss landscape initially and then fine-tuning as training progresses.</li>
          <li><i>Cyclic Learning Rate</i>: Varies the learning rate cyclically between a minimum and maximum boundary,
            which can help escape local minima and improve training performance.</li>
        </ul>
        </p>
        <p>
          <strong>Optimization Algorithms</strong> Optimization algorithms are used to adjust the weights of the model
          to minimize the loss function. Different optimizers offer various advantages depending on the problem and the
          dataset. Here are some commonly used optimizers:
        <ul>
          <li><i>Stochastic Gradient Descent (SGD)</i>: SGD updates the model's parameters using the gradient of the
            loss function with respect to the parameters for each batch of data. It is simple and effective but can be
            slow to converge and may oscillate near the minimum.</li>
          <li><i>Momentum</i>: An extension of SGD, momentum helps accelerate SGD by navigating in the relevant
            direction and dampening oscillations. It accumulates a velocity vector in the direction of the gradient's
            consistent component, speeding up the training process.</li>
          <li><i>Adagrad</i>: Adagrad adapts the learning rate for each parameter based on its gradients' historical
            sum. It is particularly useful for dealing with sparse data but can suffer from decaying learning rates over
            time.</li>
          <li><i>RMSprop</i>: RMSprop adjusts the learning rate for each parameter by dividing by a running average of
            recent gradients' magnitudes. It mitigates Adagrad's issue of decaying learning rates and performs well in
            practice.</li>
          <li><i>Adam</i>: Adam (Adaptive Moment Estimation) combines the benefits of both Adagrad and RMSprop. It
            computes adaptive learning rates for each parameter using the first and second moments of the gradients.
            Adam is widely used due to its robust performance across various tasks.</li>
          <li><i>AdamW</i>: An extension of Adam, AdamW decouples weight decay (used for regularization) from the
            gradient updates. This improves the optimizer's performance, particularly when using L2 regularization.</li>

        </ul>
        </p>
        <p>
          <strong>Batch Size</strong> The batch size is a crucial hyperparameter in deep learning training that defines
          the number of samples processed before the model's internal parameters are updated. It influences both the
          learning dynamics and computational efficiency of the training process. Choosing the right batch size involves
          balancing several trade-offs. Smaller batch sizes (e.g., 32 or 64) provide more frequent updates to the model
          parameters, which can lead to a smoother convergence and better generalization to new data. However, they may
          introduce higher noise in the gradient estimates, which can make the training process less stable. Larger
          batch sizes (e.g., 256 or 512) offer more accurate gradient estimates and can leverage parallel processing
          capabilities of modern GPUs more efficiently, potentially speeding up the training process. Yet, they require
          more memory and can lead to less frequent updates, which might result in slower convergence and risk of
          getting stuck in local minima. Empirically, a batch size that balances these factors is typically chosen based
          on the specific dataset and computational resources available. Adaptive strategies, such as progressively
          increasing the batch size during training, can also be employed to combine the benefits of both small and
          large batch sizes.
        </p>
        <p>
          <strong>Validation</strong> is a critical step in deep learning used to evaluate the model's performance on a
          separate dataset not seen during training. This dataset, called the validation set, is used to tune
          hyperparameters, select the best model, and prevent overfitting. Overfitting occurs when a model learns the
          training data too well, capturing noise and specific patterns that do not generalize to new, unseen data. This
          leads to poor performance on validation or test sets. In contrast, generalization is the model's ability to
          perform well on new, unseen data, indicating that it has learned the underlying patterns in the training data
          without memorizing it. During training, the model's performance on the validation set is monitored, and
          adjustments are made to improve generalization. This helps ensure that the model does not just memorize the
          training data but learns to generalize to new, unseen data, enhancing its robustness and applicability in
          real-world scenarios.
        </p>
        <p>
          <strong>Test</strong> The test phase, sometimes also refered to as inference, is where the trained model is
          evaluated on a completely separate dataset called the test set. This dataset is used to assess the model's
          final performance and its ability to generalize to new data. During inference, the model makes predictions on
          new data points, and its performance metrics (such as accuracy, precision, recall) are calculated. This phase
          is crucial for understanding how well the model will perform in real-world scenarios and ensures that the
          model's performance is robust and reliable.
        </p>
        <p>
          <strong>Supervised Learning</strong> is the standard approach of machine learning where the model is trained
          on labeled data. In this paradigm, the training dataset consists of input-output pairs, where each input $x$
          is associated with a known output $y$ (label). The goal of supervised learning is to learn a mapping function
          from inputs to outputs, allowing the model to make accurate predictions on new, unseen data. Supervised
          learning is widely used in various domains such as image recognition, speech recognition, and medical
          diagnosis, due to its effectiveness in learning from explicit examples.
        </p>
        <p>
          <strong>Weakly Supervised Learning</strong> is a machine learning approach where the model is trained using
          partially labeled or noisy data, as opposed to fully labeled data in traditional supervised learning. In
          weakly supervised learning, the training dataset may contain only high-level labels, partial labels, or noisy
          labels, which provide limited or ambiguous information about the ground truth. Despite the challenges posed by
          the lack of precise labels, weakly supervised learning algorithms aim to infer meaningful patterns and
          relationships from the available data to make predictions or perform tasks. This approach often requires
          innovative techniques, such as label aggregation, data augmentation, or learning from indirect supervision
          signals. Weakly supervised learning is particularly useful in scenarios where obtaining fully labeled data is
          expensive, time-consuming, or impractical, allowing models to be trained on larger, more diverse datasets.
          Additionally, weak supervision can function as implicit standardization: For example when human opinion on the
          full annotation is ambiguous, the annotations in a weak scenario might be unambiguous. Hence, the model is
          able to learn a standardization from the unambiguous weak labels.
        </p>
        <p>
          <strong>Unsupervised Learning</strong> is particularly valuable for pretraining models on large, unlabeled
          datasets. Instead of relying on labeled examples, unsupervised learning algorithms explore the raw input data
          to extract meaningful features or representations without explicit guidance. Pretraining involves training a
          model on a large amount of unlabeled data to learn general patterns and structures in the data. Once
          pretrained, the model can be fine-tuned on smaller labeled datasets for specific tasks, such as classification
          or regression. Fine-tuning adjusts the pretrained model's parameters to make it better suited for the specific
          task at hand, leveraging the knowledge gained during pretraining. Unsupervised pretraining followed by
          fine-tuning has proven to be effective in improving model performance, especially in scenarios where labeled
          data is scarce or expensive to obtain. It plays a crucial role in various applications such as natural
          language processing, computer vision, and speech recognition, enabling the development of more accurate and
          robust deep learning models.
        </p>
      </div>
    </div>
  </section>
  <!--End intro to Deep Learning  -->



  <!--Workflow -->
  <section class="section hero is-light">
    <div class="container is-max-desktop content">
      <h2 class="title is-3">Workflow</h2>
      <div class="hero-body">
        <img src="static/images/workflow.png" alt="deep learning workflow in the context of EM." />
        <h6 class="subtitle has-text-centered">
          Figure 1: We propose a simple workflow for developing deep learning solutions for the supported analysis of EM
          data.
          The workflow is clustered into three categories: 1) Task; 2) Data; 3) Model </h6>
      </div>
      <div class="content has-text-justified">
        <p>The introduced DEEP-EM TOOLBOX follows a generalizable workflow, which we introduce in the
          following.
          In our workflow (Figure 1) we propose 3 clusters:</p>
        <ul>
          <li>Task (orange).</li>
          <li>Data (green).</li>
          <li>Model (red).</li>
        </ul>

        <p>
          The standardized workflow allows easier access and realization of adaptions to the methods.
          Additionally, we identify and analyse possible challenges with applying DL to EM data and
          discuss how to tackle them.


        </p>
        <p></p>

        <div class="orange-background">
          <h3>Task</h3>

          <p>In <abbr title="Deep Learning">DL</abbr>, a task refers to a specific problem or objective that is desired
            to be addressed.
            This section will outline the necessary steps for defining tasks, providing a comprehensive foundation for
            effectively applying <abbr title="Deep Learning">DL</abbr> techniques to EM image analysis.
            Specifically, within this paper we categorize tasks in the area of <abbr
              title="Electron Microscopy">EM</abbr> into three objectives: 1) Image to Value(s), 2) Image to Image, 3)
            2D to 3D.
            Each task defines the nature of the data interactions and the desired outcomes, guiding the development and
            training of the model to perform effectively on that particular problem.</p>

          <button id="togglebtn-task-model" class="button-55"
            onclick="toggleVisibility('content-task-model', 'togglebtn-task-model')">Show More</button>
          <div id="content-task-model" class="hidden">
            <p></p>
            <h4>Definition</h4>
            <p>Task definition encompasses knowledge over the type of input data the model will process, the expected
              output or prediction, and the overall goal of the analysis. The type of input data in the case of <abbr
                title="Electron Microscopy">EM</abbr> usually corresponds to micrographs, making it well suited for the
              application of <abbr title="Deep Learning">DL</abbr> methods which originate from the area of <abbr
                title="Computer Vision">CV</abbr>. The output as well as the overall goal need to be defined
              individually for each task in mind. We title the introduced tasks based on the required type of input data
              and the expected output.</p>

            <h5>Image to Value(s)</h5>
            <p>These tasks are defined by their image input and the output of a single or multiple values. Common
              examples
              involve classification, regression, or detection.</p>
            <p>Classification in the context of <abbr title="Electron Microscopy">EM</abbr> refers to the process of
              categorizing <abbr title="Electron Microscopy">EM</abbr> images or their specific regions into predefined
              classes based on their visual characteristics. For example, it can be used to identify "good" or "bad"
              imaging regions of the sample of interest [1]. This is done by making the
              model predict a probability distribution which models the probability of the input image to belong to a
              predefined set of classes (for example, <code>C = {"good", "bad"}</code>).</p>
            <p>Regression tasks in <abbr title="Electron Microscopy">EM</abbr> refer to a type of predictive modeling
              technique used to predict a continuous output variable based on an input micrograph. Unlike
              classification, which assigns discrete classes to the input data, regression outputs a continuous value.
              This technique is particularly valuable for tasks that require quantifying certain properties of an <abbr
                title="Electron Microscopy">EM</abbr> image, such as the number of visible virus particles.</p>
            <p>Lastly, detection refers to the process of identifying and locating specific objects or features by a
              bounding box within an image. Unlike simple classification, which assigns labels to entire images, or
              regression, which predicts continuous values, detection combines both tasks: it involves pinpointing exact
              bounding boxes by regressing its position and size, as well as classifying the object located within the
              bounding box. It allows for deriving information about position, count, and sizes of the detected objects.
              This process is essential for tasks where understanding spatial distributions and feature characteristics,
              such as of virus particles within a micrograph, are critical.</p>

            <h5>Image to Image</h5>
            <p>Tasks are defined by an image input as well as an output also in the form of an image. Common examples
              involve transforming the input image into a new image representation. These tasks are fundamental in
              various applications within <abbr title="Electron Microscopy">EM</abbr>, where enhancing, restoring, or
              analyzing images is crucial for extracting valuable information from <abbr
                title="Electron Microscopy">EM</abbr> data.</p>
            <p>In denoising, the noisy input image is translated into a noise-free version. In super-resolution, a
              low-resolution micrograph is translated into a high-resolution micrograph, thereby enhancing the detail
              and clarity of the observed structures. Lastly, in segmentation, the input micrograph is translated into a
              segmented image where different regions represent distinct components, such as certain cellular organelles
              or virus particles. This involves the classification of each pixel in the input micrograph. Segments,
              which are formed by adjacent groups of uniformly classified pixels, are typically labeled, providing a
              clear distinction between different parts of the sample.</p>

            <h5>2D to 3D</h5>
            <p>Tasks are characterized by their process of converting multiple two-dimensional (2D) images into a
              three-dimensional (3D) representation. These tasks are essential in various fields, such as structural
              biology and material science, where understanding the 3D structure of samples from 2D projections is
              crucial. By integrating information from multiple 2D projections, these methods aim to produce an accurate
              and detailed 3D representation of the sample, enhancing our understanding of its spatial organization and
              functional features.</p>
            <p>Common examples correspond to <abbr title="Electron Tomography">ET</abbr>, Subtomogram Averaging, and
              Single Particle Reconstruction. In the case of <abbr title="Electron Tomography">ET</abbr> and Subtomogram
              Averaging, the input is defined by one or multiple tilt series. For Single Particle Reconstruction, the
              input corresponds to a set of picked particles.</p>

            <h4>Architecture</h4>
            <p>A model in the context of <abbr title="Deep Learning">DL</abbr> is a learnable function approximation
              based on a predefined set of trainable parameters (often also referred to as "model weights") and
              non-linear activation functions. The term "model" is often used interchangeably with the term "neural
              network" or "<abbr title="Deep Neural Network">DNN</abbr>". The learned function of a model is able to
              approximate the input-output dependencies of a set of training data.</p>
            <p>How well a function can be approximated usually depends on the model's architecture and number of
              trainable parameters. This is often referred to as the "capacity" of the model. Furthermore, a model's
              ability to generalize—i.e., to apply learned knowledge from the training data to new, unseen data—reflects
              its effectiveness and robustness. This then supports the automated analysis and interpretation of input
              <abbr title="Electron Microscopy">EM</abbr> images for tasks such as those introduced previously.
            </p>

            <h5>Backbone</h5>
            <p>The backbone of a model is the core component responsible for feature extraction and processing from the
              input data. It acts as the core architecture upon which a model is built, enabling the extraction of
              meaningful representations that can be used to perform specific tasks. The selection of a backbone is
              primarily determined by the nature of the input data and the type of task at hand.</p>
            <p>Furthermore, the choice of backbone must balance the parameter-to-data ratio. When working with limited
              datasets, it is crucial to use a model with a capacity that matches the amount of available data to avoid
              overfitting. Overfitting occurs when a model learns the training data too well, capturing noise and
              specific patterns that do not generalize to new, unseen data. To mitigate this, the model's complexity
              should be controlled according to the data's size and quality.</p>
            <p>Different backbone architectures, such as <abbr title="Multilayer Perceptron">MLP</abbr>, <abbr
                title="Convolutional Neural Network">CNN</abbr>, and <abbr title="Vision Transformer">ViT</abbr>, offer
              distinct advantages based on the data characteristics and computational constraints. Each type of backbone
              is designed to handle specific aspects of data processing, making the choice of backbone a critical factor
              in the overall model performance.</p>
            <p><em>MLPs</em> are primarily used for integrating structured or tabular data and are not typically suited
              for processing image data due to their high computational demands and risk of overfitting. They are,
              however, effective as classification heads when combined with different feature extractors.</p>
            <p><em>CNNs</em> are specifically designed for processing image data. They excel at capturing local features
              and are invariant to translations in image space. Therefore, they are particularly effective for tasks
              involving spatial relationships within images, making them ideal for <abbr
                title="Electron Microscopy">EM</abbr> image analysis.</p>
            <p>Lastly, while Transformers were originally developed for natural language processing, they were adapted
              into so-called <em>ViTs</em> to handle image data by processing image patches through self-attention
              mechanisms. They are effective at capturing global contexts and are well-suited for large datasets, where
              they usually outperform standard <abbr title="Convolutional Neural Network">CNN</abbr> architectures.
              Variants like Swin Transformers [2] and Data-efficient Image Transformers (DeiTs)
              [3] offer additional improvements for specific tasks.</p>

            <h5>Task Specific</h5>
            <p>The task-specific architecture of a model encompasses the design and arrangement of its components,
              including the types of layers, their arrangement, and the activation functions used, all of which define
              how the model processes input data to generate output.</p>
            <p>Different tasks within the realm of <abbr title="Deep Learning">DL</abbr> necessitate the use of tailored
              architectures to effectively address the unique challenges posed by each task category. This ensures the
              model is capable of accurately interpreting and processing the data to produce meaningful and reliable
              outcomes. Here, the aforementioned three task groups of Image to Value(s), Image to Image, and 2D to 3D
              have a significant impact on the model's architecture.</p>
            <p><em>Image to Value(s)</em> tasks mainly follow the use of a feature encoder (backbone) and a
              task-specific prediction head to tailor the features extracted by the backbone to better suit the specific
              task at hand. This involves transforming the abstract, high-level features into task-relevant outputs.</p>
            <p><em>Image to Image</em> tasks generally utilize encoder-decoder architectures, like U-Net
              [4]. The encoder's role is to process the input <abbr title="Electron Microscopy">EM</abbr> images and
              compress them into a lower-dimensional, abstract
              representation within the embedding/latent space. The decoder takes the compressed representation from the
              encoder and reconstructs it back to the original spatial dimensions.
            </p>
            <p>Finally, <em>2D to 3D</em> tasks are inherently complex due to their diverse interpretations and
              approaches. They can be interpreted and solved in different fashions, such that some approaches leverage
              architectures similar to Image to Value(s) task architectures and others leverage Image to Image-like
              architectures. One method [5] involves using optimization grids where no actual
              <abbr title="Deep Neural Network">DNN</abbr> is employed; instead, the 3D reconstruction is directly
              optimized. Another approach utilizes scalable data structures for efficient 3D representation
              [6]. Additionally, some techniques employ standard <abbr title="Multilayer Perceptron">MLP</abbr>-based
              architectures to estimate the density at specific
              positions in a 3D grid [7,8]<cite>kniesel2022clean, mildenhall2021nerf</cite>. Alternatively, other
              methods
              adapt Image to Image models, using a 3D decoder to reconstruct the 3D model from the encodings of multiple
              2D images.
            </p>

            <p>By meticulously defining tasks and selecting appropriate model architectures, researchers can optimize
              <abbr title="Deep Learning">DL</abbr> applications for electron microscopy and other advanced imaging
              techniques.
            </p>
            <p></p>

            <p><i>[1] Yuichi Yokoyama, Tohru Terada, Kentaro Shimizu, Kouki Nishikawa, Daisuke Kozai, Atsuhiro
              Shimada, Akira Mizoguchi, Yoshinori Fujiyoshi, and Kazutoshi Tani. Development of a deep
              learning-based method to identify “good” regions of a cryo-electron microscopy grid. Biophysical
              Reviews, 12:349–354, 2020.</i></p>
            <p><i>[2] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining
              Guo. Swin transformer: Hierarchical vision transformer using shifted windows. In Proceedings of
              the IEEE/CVF international conference on computer vision, pages 10012–10022, 2021.</i></p>
            <p><i>[3] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and
              Herv´e J´egou. Training data-efficient image transformers & distillation through attention. In
              International conference on machine learning, pages 10347–10357. PMLR, 2021</i></p>
            <p><i>[4] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedi-
              cal image segmentation. In Medical image computing and computer-assisted intervention–MICCAI
              2015: 18th international conference, Munich, Germany, October 5-9, 2015, proceedings, part III
              18, pages 234–241. Springer, 2015.</i></p>
            <p><i>[5] Animesh Karnewar, Tobias Ritschel, Oliver Wang, and Niloy Mitra. Relu fields: The little non-
              linearity that could. In ACM SIGGRAPH 2022 conference proceedings, pages 1–9, 2022</i></p>
            <p><i>[6] Alex Yu, Ruilong Li, Matthew Tancik, Hao Li, Ren Ng, and Angjoo Kanazawa. Plenoctrees
              for real-time rendering of neural radiance fields. In Proceedings of the IEEE/CVF International
              Conference on Computer Vision, pages 5752–5761, 2021</i></p>
            <p><i>[7] Hannah Kniesel, Timo Ropinski, Tim Bergner, Kavitha Shaga Devan, Clarissa Read, Paul
              Walther, Tobias Ritschel, and Pedro Hermosilla. Clean implicit 3d structure from noisy 2d
              stem images. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
              Recognition, pages 20762–20772, 2022.</i></p>
            <p><i>[8] Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik, Jonathan T Barron, Ravi Ramamoorthi,
              and Ren Ng. Nerf: Representing scenes as neural radiance fields for view synthesis. Communica-
              tions of the ACM, 65(1):99–106, 2021.</i></p>
          </div>

        </div>
        <div class="green-background">
          <h3>Data</h3>
          <p>
            Even though it is well known that large dataset sizes can drastically improve the performance of DL
            models, more data does not always equate to better model performance. High-quality data is essential
            for deriving meaningful correlations between inputs and outputs, providing a strong learning signal.
            Models must also be robust against artifacts in EM data and generalize across different sampling
            methods, detectors, and microscopes. Ensuring balanced occurrence and variance within datasets is
            crucial. Achieving optimal model performance requires balancing data quality, variance, robustness,
            and dataset size. This is vital because a trained model functions as a black box, making it difficult to
            correct biases or errors later.
          </p>
          <button id="togglebtn-data" class="button-55"
            onclick="toggleVisibility('content-data', 'togglebtn-data')">Show More</button>
          <div id="content-data" class="hidden">
            <p></p>
            <h4>Acquisition</h4>
            <h4>Annotation</h4>
            <h4>Preprocessing</h4>
          </div>

        </div>
        <div class="red-background">
          <h3> Model</h3>
          <p>
            Model training is the process of teaching a DL model to recognize patterns in data by adjusting its
            parameters to minimize prediction errors. Model evaluation assesses the trained model’s performance
            on unseen data to determine its effectiveness and generalization capabilities, ensuring that it can
            accurately predict outcomes in real-world scenarios.
          </p>
          <button id="togglebtn-model" class="button-55"
            onclick="toggleVisibility('content-model', 'togglebtn-model')">Show More</button>
          <div id="content-model" class="hidden">
            <p></p>
            <h4>Training</h4>
            <h4>Evaluation</h4>
          </div>

        </div>
      </div>
    </div>
  </section>
  <!--End workflow  -->



  <!--Links -->
  <section class="section hero">
    <div class="container is-max-desktop content">
      <h2 class="title is-3">Use Cases</h2>
      <div class="hero-body">
        <img src="static/images/tasks.png" alt="Categories of deep learning tasks in the context of EM." />
        <h6 class="subtitle has-text-centered">Tasks in the Area of EM data analysis can be categorized by the
          requirements of the DL
          method into <strong>Image to Value(s)</strong>, <strong>Image to Image</strong> and <strong>2D to 3D</strong>.
          For each category, we introduce one exemplary notebooks, tackling EM specific challenges. </h6>
      </div>
      <div class="content has-text-justified">
        <p>For providing exemplary notebooks, we are using lighting studio, as they are easy to setup, access and work
          with. A quick introduction can be found here: (https://lightning.ai/docs/overview/studios)</p>
        <p>We design our notebooks in such way, that they follow the suggested DEEP-EM TOOLBOX workflow stages.
          Within our notebooks, we further provide additional info on how to annotate and preprocess your own data in
          order to plug it into the pipeline of the use case.</p>
        <h3>Image to Value(s)</h3>
        <h4>Explainable Virus Quantification</h4>
        <h5>Challenge: Deep Learning as Black Box</h5>
        <p>Within this Image to Value(s) task, we are developing a regression model to quantify virus capsids and their
          mutation stages ("naked", "budding", "enveloped") during secondary envelopment in TEM images.
          We encourage researchers to adapt this notebook to use their own data to simplify the analysis of EM images in
          their own lab.
          For further details please see the project page.
        </p>
        <button class="button-55"
          onclick="location.href='https://viscom-ulm.github.io/DeepEM/explainable-virus-quantification';">
          Go to Project
        </button>
        <figure>
          <img src="static/images/explainable-virus-quantification/Teaser.png"
            alt="Teaser explainable virus quantification">
          <figcaption id="fig:teaser-ensemble">
            For the explainable virus quantification we train a regression model to predict the number of "naked",
            "budding" and "enveloped" virus capsids in the input image.
            We use GradCAM as an explainable AI technique, to make the model more trustworthy and the predictions
            easier to coprehend.
          </figcaption>
        </figure>


        <hr>


        <h3>Image to Image</h3>
        <h4>Segmentation of Cellular Structures</h4>
        <h5>Challenge: Robustness with small dataset sizes</h5>

        <p>We choose the segmentation of certain cell organelles as a relevant Image to Image task.
          Segmentation is an important tool in EM image analysis as it contributes to a better visualisation of
          certain
          organelles and complex cell structures,
          which facilitates the interpretation of EM data. Segmentation allows for detailed analysis of organelle
          morphology, spatial relationships and distribution within cells.
          This is crucial for understanding intracellular organisation and its relationship to cell function.
          Due to the small available dataset sizes we deploy data augmentation methods, make use of pretrained weights
          and train an ensemble model, which has been shown to provide
          better generalizability even when trained on smaller dataset sizes [1].
          For further details please see the project page.
        </p>
        <button class="button-55"
          onclick="location.href='https://viscom-ulm.github.io/DeepEM/segmentation-of-cellular-structures';">
          Go to Project
        </button>
        <figure>
          <img src="static/images/segmentation-of-cellular-structures/Teaser-ensemble.png"
            alt="Depiction of ensemble model">
          <figcaption id="fig:teaser-ensemble">
            For the segmentation of cellular structures we follow [1] and train a so called "ensemble" model.
            An ensemble model is a set of models which are used to make multiple predictions for the same input data.
            The predictions are then combined to retrieve a more robust prediction.

          </figcaption>
        </figure>
        <p><i>[1] Shaga Devan, Kavitha, et al. "Weighted average ensemble-based semantic segmentation in biological
            electron microscopy images." Histochemistry and Cell Biology 158.5 (2022): 447-462.</i></p>


        <hr>

        <h3>2D to 3D</h3>
        <h4>Deep Learning Based Tomographic Reconstruction of Scanning Transmission Electron Microscopy (STEM) Images
        </h4>
        <h5>Challenge: Evaluation with missing ground truth</h5>
        <p>To introduce 2D to 3D tasks, we implement a learning based tomographic reconstruction of 2D projections
          obtained from STEM tomograms, following [1,2].
          Reconstruction of a 3D volume allows visualisation of the true morphology, spatial relationships and
          connectivity of certain cellular structures and
          organelles within a cell, which may not be visible in 2D projections alone. Due to missing ground truth
          information in the case of tomographic reconstruction,
          we make use of pre-existing synthetic data to assess the model performance.
          For further details please see the project page.
        </p>
        <button class="button-55"
          onclick="location.href='https://viscom-ulm.github.io/DeepEM/tomographic-reconstruction-stem';">
          Go to Project
        </button>
        <figure>
          <img src="static/images/tomographic-reconstruction-stem/Teaser.gif"
            alt="Depiction of the tomographic reconstruction. (Show tilt series, model, reconstruction of Nanoparticles)">
          <figcaption id="fig:Tomo">
            Based on a given tilt series, we are able to generate a 3D reconstruction using self-supervised deep
            learning.
          </figcaption>
        </figure>


        <p></p>
        <p><i>[1] Kniesel, Hannah, et al. "Clean implicit 3d structure from noisy 2d stem images." Proceedings of the
            IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2022.</i></p>
        <p><i>[2] Mildenhall, Ben, et al. "Nerf: Representing scenes as neural radiance fields for view synthesis."
            Communications of the ACM 65.1 (2021): 99-106.</i></p>

        <hr>

      </div>

    </div>
  </section>
  <!--End Links  -->

  <!--BibTex citation -->
  <section class="section hero">
    <div class="container is-max-desktop content">
      <h2 class="title">Your Turn!</h2>
      <p>
        We further encourage contributions from the research community.
        We ask you to test and use the provided notebooks within your own lab and adapt them to your needs.
        We appreciate any further contributions of use cases to make them easily accessible to the EM research
        community.


      </p>
      <p>Please don't hesitate to contact us!</p>
    </div>
  </section>
  <!--End BibTex citation -->



  <!--BibTex citation -->
  <section class="section hero" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>BibTex Code Here</code></pre>
    </div>
  </section>
  <!--End BibTex citation -->

  <!-- Footer  -->
  <footer class="footer">
    <div class="container">
      <div class="columns is-centered">
        <div class="column is-8">
          <div class="content">

            <p>
              This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template"
                target="_blank">Academic Project Page Template</a> which was adopted from the <a
                href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
              You are free to borrow the of this website, we just ask that you link back to this page in the footer.
              <br> This website is licensed under a <a rel="license"
                href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
                Commons Attribution-ShareAlike 4.0 International License</a>.
            </p>

          </div>
        </div>
      </div>
    </div>
  </footer>
  <!-- End footer -->

  <!-- Statcounter tracking code -->

  <!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

  <!-- End of Statcounter Code -->

</body>

</html>